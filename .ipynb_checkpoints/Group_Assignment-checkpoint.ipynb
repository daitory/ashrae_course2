{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GROUP ASSIGNMENT - STATISTICS FOR DATA SCIENCE - FALL 2019 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members: Darwing Cara, David Kobayashi, Eric Koritko, Stefan Lazarevic, Patrick McDonnell ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have selected to work on a data set from ASHRAE (a technical society for heating, ventilation, and air conditioning in buildings). The dataset can be found here: https://www.kaggle.com/c/ashrae-energy-prediction\n",
    "\n",
    "The goal of this exercise is to use the dataset to build a model that can predict energy use in a building based on that building's features (building use type, outdoor air conditions, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - IMPORT PACKAGES ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import math as math\n",
    "import datetime\n",
    "import time\n",
    "from dateutil.parser import parse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - DEFINE USEFUL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_dataframe(dataframe):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    print('Dataframe was {:.2f} MB'.format(startingMemoryUsage))\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        columnDataType = dataframe[column].dtype\n",
    "        \n",
    "        if column == 'timestamp':\n",
    "            dataframe[column] = pd.to_datetime(dataframe[column], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        elif columnDataType != object:\n",
    "            columnMin = dataframe[column].min()\n",
    "            columnMax = dataframe[column].max()\n",
    "\n",
    "            if str(columnDataType)[:3] == 'int':\n",
    "                if columnMin > np.iinfo(np.int8).min and columnMax < np.iinfo(np.int8).max:\n",
    "                    dataframe[column] = dataframe[column].astype(np.int8)\n",
    "                elif columnMin > np.iinfo(np.int16).min and columnMax < np.iinfo(np.int16).max:\n",
    "                    dataframe[column] = dataframe[column].astype(np.int16)\n",
    "                elif columnMin > np.iinfo(np.int32).min and columnMax < np.iinfo(np.int32).max:\n",
    "                    dataframe[column] = dataframe[column].astype(np.int32)\n",
    "                elif columnMin > np.iinfo(np.int64).min and columnMax < np.iinfo(np.int64).max:\n",
    "                    dataframe[column] = dataframe[column].astype(np.int64)  \n",
    "            else:\n",
    "                if columnMin > np.finfo(np.float16).min and columnMax < np.finfo(np.float16).max:\n",
    "                    dataframe[column] = dataframe[column].astype(np.float16)\n",
    "                elif columnMin > np.finfo(np.float32).min and columnMax < np.finfo(np.float32).max:\n",
    "                    dataframe[column] = dataframe[column].astype(np.float32)\n",
    "                else:\n",
    "                    dataframe[column] = dataframe[column].astype(np.float64)\n",
    "        else:\n",
    "            dataframe[column] = dataframe[column].astype('category')\n",
    "\n",
    "    endingingMemoryUsage = dataframe.memory_usage().sum() / 1024**2\n",
    "\n",
    "    print('Dataframe is now: {:.2f} MB'.format(endingingMemoryUsage))\n",
    "    \n",
    "    print(\"Time to reduce dataframe size:\",round((time.time()-start)/60, 2), 'minutes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a building_id, a meter type, and a date range, plot the energy use\n",
    "\n",
    "def plotData(df, building_id, meter, name, start_day, end_day):\n",
    "  \n",
    "    df_plt = df[(df['building_id'] == building_id) & (df['meter'] == meter)]\n",
    "    df_plt = df_plt[(df_plt['timestamp'] > start_day) & (df_plt['timestamp'] < end_day)]\n",
    "    \n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(df_plt['timestamp'], df_plt['meter_reading'])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Meter Reading')\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a zone_id and a date range, plot the weather\n",
    "\n",
    "def plotWeather(df, zone_id, name, start_day, end_day, parameter):\n",
    "\n",
    "    df_plt = df[df['site_id'] == zone_id]\n",
    "    df_plt = df_plt[(df_plt['timestamp'] > start_day) & (df_plt['timestamp'] < end_day)]\n",
    "    \n",
    "    plt.figure(figsize=(20,2))\n",
    "    plt.plot(df_plt['timestamp'], df_plt[parameter])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a building_id, plot energy use (all three meters, if available) and weather data\n",
    "\n",
    "def display_energy_use(df, building_id, start_day, end_day):\n",
    "\n",
    "    print(\"Displaying data for Building\", building_id, \"between:\", start_day, \"and\", end_day, '\\n')\n",
    "    \n",
    "    print(\"Building's primary use:\", df.loc[df['building_id'] == building_id, 'primary_use'].iloc[0])\n",
    "    \n",
    "    print(\"Building size:\", df.loc[df['building_id'] == building_id, 'square_feet'].iloc[0], 'square feet')\n",
    "      \n",
    "    for meter_type in df[df['building_id'] == building_id].groupby('meter').meter.unique():\n",
    "        if(meter_type == elec_meter):\n",
    "            plotData(df, building_id, elec_meter, \"Electricity Meter Data\", start_day, end_day)\n",
    "        if(meter_type == chw_meter):\n",
    "            plotData(df, building_id, chw_meter,\"Chilled Water Meter Data\", start_day, end_day)\n",
    "        if(meter_type == steam_meter):\n",
    "            plotData(df, building_id, steam_meter, \"Steam Meter Data\", start_day, end_day)\n",
    "        if(meter_type == hw_meter):\n",
    "            plotData(df, building_id,hw_meter, \"Hot Water Meter Data\", start_day, end_day)            \n",
    "            \n",
    "    plotWeather(df, df.loc[df['building_id'] == building, 'site_id'].iloc[0], \"Weather Data - Air Temperature\", start_day, end_day, 'air_temperature')\n",
    "    plotWeather(df, df.loc[df['building_id'] == building, 'site_id'].iloc[0], \"Weather Data - Cloud Coverage\", start_day, end_day, 'cloud_coverage')\n",
    "    plotWeather(df, df.loc[df['building_id'] == building, 'site_id'].iloc[0], \"Weather Data - Dewpoint Temperature\", start_day, end_day, 'dew_temperature')\n",
    "    plotWeather(df, df.loc[df['building_id'] == building, 'site_id'].iloc[0], \"Weather Data - Precipitation Depth\", start_day, end_day, 'precip_depth_1_hr')\n",
    "    plotWeather(df, df.loc[df['building_id'] == building, 'site_id'].iloc[0], \"Weather Data - Sea Level Pressure\", start_day, end_day, 'sea_level_pressure')\n",
    "    plotWeather(df, df.loc[df['building_id'] == building, 'site_id'].iloc[0], \"Weather Data - Wind Direction\", start_day, end_day, 'wind_direction')\n",
    "    plotWeather(df, df.loc[df['building_id'] == building, 'site_id'].iloc[0], \"Weather Data - Wind Speed\", start_day, end_day, 'wind_speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - DEFINE USEFUL CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "elec_meter = 0\n",
    "chw_meter = 1\n",
    "steam_meter = 2\n",
    "hw_meter = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 - LOAD DATASET INTO MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'train.csv' does not exist: b'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2136c64af9f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time to read the CSV file into a dataframe: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seconds \\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'train.csv' does not exist: b'train.csv'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "df = pd.read_csv(filepath_or_buffer='train.csv', sep=',', low_memory=False)\n",
    "\n",
    "print('Time to read the CSV file into a dataframe: ',round((time.time()-start), 2), 'seconds \\n')\n",
    "\n",
    "startingMemoryUsage = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "print('Dataframe size is {:.2f} MB'.format(startingMemoryUsage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_weather = pd.read_csv(filepath_or_buffer='weather_train.csv', sep=',', low_memory=False)\n",
    "\n",
    "print('Time to read the CSV file into a dataframe: ',round((time.time()-start), 2), 'seconds \\n')\n",
    "\n",
    "startingMemoryUsage = df_weather.memory_usage().sum() / 1024**2\n",
    "\n",
    "print('Dataframe size is {:.2f} MB'.format(startingMemoryUsage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_dataframe(df_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df_building_metadata = pd.read_csv(filepath_or_buffer='building_metadata.csv', sep=',', low_memory=False)\n",
    "\n",
    "print('Time to read the CSV file into a dataframe: ',round((time.time()-start), 2), 'seconds \\n')\n",
    "\n",
    "startingMemoryUsage = df_weather.memory_usage().sum() / 1024**2\n",
    "\n",
    "print('Dataframe size is {:.2f} MB'.format(startingMemoryUsage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_building_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_building_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_dataframe(df_building_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 - CLEAN THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - CHECK FOR DUPLICATE ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - CHECK FOR NULL VALUES IN THE MAIN DATASET AND WEATHER DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many nulls there are in the main dataset (expressed as a percentage)\n",
    "print(round(df.isna().sum().sort_values(ascending=False) / len(df.index) * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many nulls there are in the weather dataset (expressed as a percentage)\n",
    "print(round(df_weather.isna().sum().sort_values(ascending=False) / len(df_weather.index) * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nulls with linear interpolation\n",
    "df_weather = df_weather.interpolate()\n",
    "df_weather = df_weather.fillna(0)\n",
    "\n",
    "# Double check how many nulls are in the feature (should be 0 now)\n",
    "print(round(df_weather.isna().sum().sort_values(ascending=False) / len(df_weather.index) * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many nulls there are in the metadata dataset (expressed as a percentage)\n",
    "print(round(df_building_metadata.isna().sum().sort_values(ascending=False) / len(df_building_metadata.index) * 100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - FILTER OUT OFFICE BUILDINGS AND MERGE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of different building types in the dataset\n",
    "# df_building_metadata['primary_use'].value_counts()\n",
    "\n",
    "# filter out the data that relates to office buildings\n",
    "df_building_metadata = df_building_metadata[df_building_metadata['primary_use'] == 'Office']\n",
    "\n",
    "# merge this data with the main dataframe, so that we only have meter readings for office buildings\n",
    "df = df.merge(df_building_metadata, on='building_id', how='inner')\n",
    "\n",
    "# merge weather data with the main dataframe, so that all the information is in one place\n",
    "df = df.merge(df_weather, on=['site_id', 'timestamp'], how='inner')\n",
    "\n",
    "# check dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - CHECK FOR NULL VALUES IN THE METADATA DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(df.isna().sum().sort_values(ascending=False) / len(df.index) * 100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# There's no great way to fill the floor_count and year_built columns\n",
    "# Since there are a large number of nulls in the floor_count and year_built features, we will drop these columns\n",
    "\n",
    "df = df.drop('floor_count', axis=1)\n",
    "df = df.drop('year_built', axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 - USE THIS SECTION TO VIEW METER DATA AND WEATHER DATA TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Change the building_id, the start date, and the end date. This will plot everything, in increments of days\n",
    "\n",
    "# --------------------------#\n",
    "building = 9\n",
    "start_day = \"2016-07-01\"\n",
    "end_day = \"2016-07-02\"\n",
    "# --------------------------#\n",
    "\n",
    "if building in df['building_id'].unique():\n",
    "    display_energy_use(df, building, start_day, end_day)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - PERFORM VARIABLE SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - EXAMINE PRIMARY USE COLUMN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['primary_use'].describe())\n",
    "\n",
    "# We can see that the Primary Use column, has only a single unique value (Office).\n",
    "\n",
    "# This is the result of the data cleaning phase, that resulted in extracting the results for the office buildings only.\n",
    "\n",
    "# To that end, this feature is not very useful for us, since all rows represent the Office Buildings data, and we can remove it\n",
    "# from the Data Set:\n",
    "\n",
    "df = df.drop(['primary_use'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - EXAMINE ENERGY CONSUMPTION OVER THE YEAR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's transform the meter column s.t. it has actual energy type name as opposed to different numbers. This will make\n",
    "# the Data Set more clear.\n",
    "\n",
    "# Every row in the column has a different number representing the different energy type:\n",
    "\n",
    "# 0 - electricity\n",
    "# 1 - chilledwater\n",
    "# 2 - steam\n",
    "# 3 - hotwater\n",
    "\n",
    "df['meter'].replace( { 0: 'Electricity', 1: 'ChilledWater' ,2: 'Steam', 3: 'HotWater' }, inplace=True )\n",
    "\n",
    "\n",
    "# Now, examine the timestamp column:\n",
    "\n",
    "print(df['timestamp'].head(10))\n",
    "\n",
    "# Extract the year from each timestamp:\n",
    "\n",
    "df['year'] = df['timestamp'].dt.year.astype('uint8')\n",
    "\n",
    "# Examine the different years for which the data was collected:\n",
    "\n",
    "print('The number of different years the data was collected for: ' + str(df['year'].nunique()))\n",
    "\n",
    "# Based on our findings, we can see that the data was collected for the year of 2016 only. Thus, we will extract the different\n",
    "# month values from each timestamp, and drop the 'year' column, as it is not very useful for us:\n",
    "\n",
    "df = df.drop(['year'], axis=1)\n",
    "\n",
    "# Extract the month column:\n",
    "\n",
    "df['month'] = df['timestamp'].dt.month.astype('uint8')\n",
    "\n",
    "# Examine the month column:\n",
    "\n",
    "print('The number of different months the data was collected for: ' + str(df['month'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of different types of energy in the Data Set using the countplot graph:\n",
    "\n",
    "sns.set()\n",
    "sns.countplot(df['meter'],order=df['meter'].value_counts().sort_values().index)\n",
    "plt.title(\"Count Distribution of different Energey Types\")\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Based on our countplot, we can see that Electricity has the highest frequency, followed by ChilledWater.\n",
    "# Steam has lower frequency than the two above, and HotWater has the lowest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how the energy consumption is distributed for the different energy types:\n",
    "\n",
    "df.groupby('meter')['meter_reading'].agg(['min','max','mean','median','count','std'])\n",
    "\n",
    "# Based on our findings, we can see that Steam has a much higher consumption as opposed to the other energy types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot to see any trends in energy consumption throughout the year:\n",
    "\n",
    "df[['timestamp','meter_reading']].set_index('timestamp').resample('H')['meter_reading'].mean().plot(kind='line',figsize=(10,6),label='Average Energy Consumption throughout the Year:')\n",
    "plt.legend()\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Average Energy Consumption')\n",
    "plt.title('Graph of Averagy Energy Consumption throughout the year: ')\n",
    "\n",
    "# Based on our graph, we can see that the energy consumption is the highest in the months of January-February, July-August,\n",
    "# and on the month of December.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the energy consumption throughout the year for each energy type:\n",
    "\n",
    "# Define a function to plot the average energy consumption for individual energent:\n",
    "def consumption_grapher(meter_value):\n",
    "    energy_type = df[df['meter'] == meter_value]\n",
    "    energy_type[['timestamp','meter_reading']].set_index('timestamp').resample('H')['meter_reading'].mean().plot(kind='line',figsize=(10,6),label='Average Meter Reading')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Average Meter Reading')\n",
    "    plt.title('Graph of Average Meter Reading for ' + meter_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Electricity Consumption throughout the year:\n",
    "\n",
    "consumption_grapher('Electricity')\n",
    "\n",
    "# We can see that the Electricity consumption generally increases throughout the year until the month of October, when it\n",
    "# starts decreasing for the rest of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the ChilledWater consumption throughout the year:\n",
    "\n",
    "consumption_grapher('ChilledWater')\n",
    "\n",
    "# We can see a large increase in ChilledWater consumption from the months of January-August when it is the highest.\n",
    "# The consumption then starts dropping, and records the lowest values in the month of December."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Steam consumption throughout the year:\n",
    "\n",
    "consumption_grapher('Steam')\n",
    "\n",
    "# We can see that the Steam consumption generally follows the similar trend as the trend of all energy types combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the HotWater consumption throughout the year:\n",
    "\n",
    "consumption_grapher('HotWater')\n",
    "\n",
    "# We can see that the HotWater consumption also follows the similar trend to the trend of all energy types combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupby to calculate the min, max, median, count, and std values for each energent for each different month:\n",
    "\n",
    "df.groupby(['meter','month'])['meter_reading'].agg(['max','mean','median','count','std'])\n",
    "\n",
    "# Based on our findings, we can see that the Steam consumption is generally a lot higher than that of the other energy types\n",
    "# for the different months, and has lower counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we cannot see any clear distribution in the energy consumption, we will transform the meter_reading using\n",
    "# log transformation:\n",
    "\n",
    "df['meter_reading'] = np.log1p(df['meter_reading'])\n",
    "\n",
    "# Use the distribution plot to see how the log-transformed energy consumption is distributed:\n",
    "\n",
    "sns.distplot(df['meter_reading'])\n",
    "plt.title(\"Distribution of Log-transformed of Meter Reading Variable\")\n",
    "\n",
    "# Based on the graph we can see that the log-transformed energy consumption follows a nearly normal distribution, however\n",
    "# it seems to have a lot of 0-value outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the outliers using the Box Plot graphs:\n",
    "\n",
    "energy_types = [ 'Electricity', 'ChilledWater', 'HotWater', 'Steam' ]\n",
    "\n",
    "def boxplot_grapher(meter_type):\n",
    "    sns.boxplot(df[df['meter'] == meter_type]['meter_reading'])\n",
    "    plt.title('Boxplot of Meter Reading Variable for the Meter Type: ' + meter_type)\n",
    "\n",
    "# Based on our Box Plots, we can see that there are a lot of 0-valued outliers, and that Electricity has the outliers in\n",
    "# values greater than 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_grapher('Electricity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_grapher('ChilledWater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_grapher('HotWater')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_grapher('Steam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our findings, we can determine the following:\n",
    "\n",
    "# 1) Despite a relatively low count, the Energy Consumption of Steam is much higher than that of the other Energy Types,\n",
    "#    to the extend where it dictates the trends in the energy consumption. To that end we will remove it from the Data Set.\n",
    "\n",
    "# 2) The Data Set appears to have a lot of 0-valued outliers, and we will be removing these:\n",
    "\n",
    "# Remove the 0-valued outliers:\n",
    "df.drop(df.loc[(df['meter'] == 'Electricity') & (df['meter_reading'] == 0)].index, inplace=True)\n",
    "df.drop(df.loc[(df['meter'] == 'HotWater') & (df['meter_reading'] == 0)].index, inplace=True)\n",
    "df.drop(df.loc[(df['meter'] == 'ChilledWater') & (df['meter_reading'] == 0)].index, inplace=True)\n",
    "\n",
    "# Remove the outliers from Electricity with values greater than 8:\n",
    "df.drop(df.loc[(df['meter'] == 'Electricity') & (df['meter_reading'] > 8)].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Steam from the Data Set:\n",
    "df.drop(df.loc[df['meter']=='Steam'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the Energy Distribution throughout the year after implementing the transformation:\n",
    "\n",
    "sns.distplot(df['meter_reading'])\n",
    "plt.title(\"Distribution of Log-transformed of Meter Reading Variable\")\n",
    "\n",
    "# Based on the graph below, the meter reading now follows the normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 EXAMINE THE WEATHER FEATURES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an array to hold the column names of all weather features:\n",
    "weather_columns = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_speed', 'wind_direction']\n",
    "\n",
    "# Examine the weather data:\n",
    "df[weather_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Distribution Plot to see how the different Weather Features are distributed:\n",
    "\n",
    "for x,column in enumerate(df[weather_columns]):\n",
    "    plt.figure(x)\n",
    "    sns.distplot(df[column])\n",
    "    \n",
    "# Based on our findings, we can determine the following:\n",
    "\n",
    "# air_temperature appears to be positively-skewed.\n",
    "# cloud_coverage is unique in that it is composed of a number of distinct values.\n",
    "# dew_temperature appears to be positively-skewed.\n",
    "# percip_depth_1_hr has a lot of 0 values in the Data Set.\n",
    "# sea_level_pressure follows the normal distribution.\n",
    "# wind_speed seems to be negatively skewed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will examine the columns to see if they have a very high level of coordination:\n",
    "\n",
    "# Threshold for removing correlated variables\n",
    "threshold = 0.9\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = df.corr().abs()\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns with very high levels of correlation to be removed\n",
    "columns = [column for column in corr_matrix[weather_columns] if any(corr_matrix[column] > threshold)]\n",
    "\n",
    "print (\"Columns with high levels of correlation to be removed {}\".format(columns))\n",
    "\n",
    "# We will keep the sea_level_pressure, as it follows the normal distribution, and can be used for energy prediction:\n",
    "\n",
    "columns.remove('sea_level_pressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our findings, the below columns have a very high levels of correlation, and thus can be removed:\n",
    "\n",
    "# air_temperature\n",
    "# cloud_coverage\n",
    "# dew_temperature\n",
    "# precip_depth_1_hr\n",
    "# wind_speed\n",
    "\n",
    "\n",
    "df = df.drop(columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 EXAMINE THE SURFACE AREA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['square_feet'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Distribution Plot to see how the Surface Area is distributed:\n",
    "\n",
    "sns.distplot(df['square_feet'])\n",
    "plt.title(\"Distribution of Building's Surface Area: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Surface Area seems to be negatively-skewed.\n",
    "\n",
    "# Let's transform it using the log based 10 transformation:\n",
    "\n",
    "df['square_feet'] = np.log10(df['square_feet'])\n",
    "\n",
    "# We can see that after the log-transformation the Surface Area follows an almost Normal Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['square_feet'])\n",
    "plt.title(\"Distribution of Building's Surface Area: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 REMOVE FEATURES THAT ARE NO LONGER NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can remove the features that are no longer needed:\n",
    "\n",
    "# The site_id and building_id are used as unique identifiers for buildings and sites they are located, and are thus not very\n",
    "# useful for predicting energy.\n",
    "\n",
    "# Likewise, we no longer need the timestamp and time features, as they were used for transforming data.\n",
    "\n",
    "df = df.drop(['month', 'timestamp', 'site_id', 'building_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
